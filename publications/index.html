<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Gasper Begus | publications</title>
<meta name="description" content="Assistant Professor of Linguistics at UC Berkeley. Deep learning and computation in speech, phonetics, phonology.
">

<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/css/mdb.min.css" integrity="sha256-/SwJ2GDcEt5382i8zqDwl36VJGECxEoIcBIuoLmLR4g=" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"  integrity="sha256-h20CPZ0QyXlBuAw7A+KluUYx/3pK+c7lYEpqLTlxjYQ=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Styles -->
<link rel="shortcut icon" href="/assets/img/favicon.ico">
<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/publications/">

<!-- Open Graph -->


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light bg-white navbar-expand-sm fixed-top">
    <div class="container">
      
      
      
      
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       <span class="font-weight-bold">Gasper</span>   Begus
      </a>
      
      <!-- Navbar Toogle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/CV/">
                CV
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching/advising
                
              </a>
          </li>
          
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description">publications in reversed chronological order.</p>
  </header>

  <article>
    <div class="publications">


  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus20" class="col-sm-8">
    
      <span class="title">Generative Adversarial Phonology: Modeling Unsupervised Phonetic and Phonological Learning With Neural Networks</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Frontiers in Artificial Intelligence</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.frontiersin.org/article/10.3389/frai.2020.00044" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusGAP.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Training deep neural networks on well-understood dependencies in speech data can provide new insights into how they learn internal representations. This paper argues that acquisition of speech can be modeled as a dependency between random space and generated speech data in the Generative Adversarial Network architecture and proposes a methodology to uncover the network’s internal representations that correspond to phonetic and phonological properties. The Generative Adversarial architecture is uniquely appropriate for modeling phonetic and phonological learning because the network is trained on unannotated raw acoustic data and learning is unsupervised without any language-specific assumptions or pre-assumed levels of abstraction. A Generative Adversarial Network was trained on an allophonic distribution in English, in which voiceless stops surface as aspirated word-initially before stressed vowels, except if preceded by a sibilant [s]. The network successfully learns the allophonic alternation: the network’s generated speech signal contains the conditional distribution of aspiration duration. The paper proposes a technique for establishing the network’s internal representations that identifies latent variables that correspond to, for example, presence of [s] and its spectral properties. By manipulating these variables, we actively control the presence of [s] and its frication amplitude in the generated outputs. This suggests that the network learns to use latent variables as an approximation of phonetic and phonological representations. Crucially, we observe that the dependencies learned in training extend beyond the training interval, which allows for additional exploration of learning representations. The paper also discusses how the network’s architecture and innovative outputs resemble and differ from linguistic behavior in language acquisition, speech disorders, and speech errors, and how well-understood dependencies in speech data can help us interpret how neural networks learn their representations.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusscil" class="col-sm-8">
    
      <span class="title">Modeling unsupervised phonetic and phonological learning in Generative Adversarial Phonology</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the Society for Computation in Linguistics</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://doi.org/10.7275/nbrf-1a27" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusScil.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper models phonetic and phonological learning as a dependency between random space and generated speech data in the Generative Adversarial Neural network architecture and proposes a methodology to uncover the network’s internal representation that corresponds to phonetic and phonological features. A Generative Adversarial Network (Goodfellow et al. 2014; implemented as WaveGAN for acoustic data by Donahue et al. 2019) was trained on an allophonic distribution in English, where voiceless stops surface as aspirated word-initially before stressed vowels except if preceded by a sibilant [s]. The network successfully learns the allophonic alternation: the network’s generated speech signal contains the conditional distribution of aspiration duration. Additionally, the network generates innovative outputs for which no evidence is available in the training data, suggesting that the network segments continuous speech signal into units that can be productively recombined. The paper also proposes a technique for establishing the network’s internal representations. We identify latent variables that directly correspond to presence of [s] in the output. By manipulating these variables, we actively control the presence of [s], its frication amplitude, and spectral shape of the frication noise in the generated outputs.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begu2020ciwgan" class="col-sm-8">
    
      <span class="title">CiwGAN and fiwGAN: Encoding information in acoustic data to model lexical learning with Generative Adversarial Networks</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2006.02951" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus2006.02951.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>How can deep neural networks encode information that corresponds to words in human speech into raw acoustic data? This paper proposes two neural network architectures for modeling unsupervised lexical learning from raw acoustic inputs, ciwGAN (Categorical InfoWaveGAN) and fiwGAN (Featural InfoWaveGAN), that combine a DCGAN architecture for audio data (WaveGAN; arXiv:1705.07904) with InfoGAN (arXiv:1606.03657), and propose a new latent space structure that can model featural learning simultaneously with a higher level classification. The architectures introduce a network that learns to retrieve latent codes from generated audio outputs. Lexical learning is thus modeled as emergent from an architecture that forces a deep neural network to output data such that unique information is retrievable from its acoustic outputs. The networks trained on lexical items from TIMIT learn to encode unique information corresponding to lexical items in the form of categorical variables. By manipulating these variables, the network outputs specific lexical items. Innovative outputs suggest that phonetic and phonological representations learned by the network can be productively recombined and directly paralleled to productivity in human speech: a fiwGAN network trained on ’suit’ and ’dark’ outputs innovative ’start’, even though it never saw ’start’ or even a [st] sequence in the training data. We also argue that setting latent featural codes to values well beyond training range results in almost categorical generation of prototypical lexical items and reveals underlying values of each latent code. Probing deep neural networks trained on well understood dependencies in speech bears implications for latent space interpretability, understanding how deep neural networks learn meaningful representations, as well as a potential for unsupervised text-to-speech generation in the GAN framework.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begu2020id" class="col-sm-8">
    
      <span class="title">Identity-Based Patterns in Deep Convolutional Networks: Generative Adversarial Phonology and Reduplication</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2009.06110" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus2009.06110.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Identity-based patterns for which a computational model needs to output some feature together with a copy of that feature are computationally challenging, but pose no problems to human learners and are common in world’s languages. In this paper, we test whether a neural network can learn an identity-based pattern in speech called reduplication. To our knowledge, this is the first attempt to test identity-based patterns in deep convolutional networks trained on raw continuous data. Unlike existing proposals, we test learning in an unsupervised manner and we train the network on raw acoustic data. We use the ciwGAN architecture (Beguš 2020; arXiv:2006.02951) in which learning of meaningful representations in speech emerges from a requirement that the deep convolutional network generates informative data. Based on four generative tests, we argue that a deep convolutional network learns to represent an identity-based pattern in its latent space; by manipulating only two categorical variables in the latent space, we can actively turn an unreduplicated form into a reduplicated form with no other changes to the output in the majority of cases. We also argue that the network extends the identity-based pattern to unobserved data: when reduplication is forced in the output with the proposed technique for latent space manipulation, the network generates reduplicated data (e.g., it copies an [s] e.g. in [si-siju] for [siju] although it never sees any reduplicated forms containing an [s] in the input). Comparison with human outputs of reduplication show a high degree of similarity. Exploration of how meaningful representations of identity-based patterns emerge and how the latent space variables outside of the training range correlate with identity-based patterns in the output has general implications for neural network interpretability.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begu2020local" class="col-sm-8">
    
      <span class="title">Local and non-local dependency learning and emergence of rule-like representations in speech data by Deep Convolutional Generative Adversarial Networks</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2009.12711" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusLocal.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper argues that training Generative Adversarial Networks (GANs) on local and non-local dependencies in speech data offers insights into how deep neural networks discretize continuous data and how symbolic-like rule-based morphophonological processes emerge in a deep convolutional architecture. Acquisition of speech has recently been modeled as a dependency between latent space and data generated by GANs in Beguš (2020c), who models learning of a simple local allophonic distribution. We extend this approach to test learning of local and non-local phonological processes that include approximations of morphological processes. We further parallel outputs of the model to results of a behavioral experiment where human subjects are trained on the data used for training the GAN network. Four main conclusions emerge: (i) the networks provide useful information for computational models of language acquisition even if trained on a comparatively small dataset of an artificial grammar learning experiment; (ii) local processes are easier to learn than non-local processes, which matches both behavioral data in human subjects and typology in the world’s languages. This paper also proposes (iii) how we can actively observe the network’s progress in learning and explore the effect of training steps on learning representations by keeping latent space constant across different training steps. Finally, this paper shows that (iv) the network learns to encode the presence of a prefix with a single latent variable; by interpolating this variable, we can actively observe the operation of a non-local phonological process. The proposed technique for retrieving learning representations has general implications for our understanding of how GANs discretize continuous speech data and suggests that rule-like generalizations in the training data are represented as an interaction between variables in the network’s latent space.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begu2020artificial" class="col-sm-8">
    
      <span class="title">Artificial sound change: Language change and deep convolutional neural networks in iterative learning</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://arxiv.org/abs/2011.05463" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusArtificial.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper proposes a framework for modeling sound change that combines deep convolutional neural networks and iterative learning. Acquisition and transmission of speech across generations is modeled by training generations of Generative Adversarial Networks on unannotated raw speech data. The paper argues that several properties of sound change emerge from the proposed architecture. Generative Adversarial Networks (Goodfellow et al., 2014; Donahue et al., 2019) are uniquely appropriate for modeling language change because the networks are trained on raw unsupervised acoustic data, contain no language-specific devices and, as argued in Beguˇs (2020b), encode phonetic and phonological representations in their latent space and generate innovative data that are linguistically highly informative. Four generations of Generative Adversarial Networks were trained on an allophonic distribution in English where voiceless stops are aspirated word-initially before stressed vowels except if preceded by [s]. The first generation of networks is trained on the relevant sequences in human speech from the TIMIT database. The subsequent generations are not trained on TIMIT, but on generated outputs from the previous generation and thus start learning from each other in an iterative learning task. The initial allophonic distribution is progressively being lost with each generation, likely due to pressures from the global distribution of aspiration in the training data that resembles phonological pressures in natural language. The networks show signs of a gradual shift in phonetic targets characteristic of a gradual phonetic sound change. At endpoints, the networks’ outputs superficially resemble a phonological change — rule loss — driven by imperfect learning. The model features signs of stability, one of the more challenging aspects of computational models of sound change. The results suggest that the proposed Generative Adversarial models of phonetic and phonological acquisition have the potential to yield new insights into the long-standing question of how to model language change.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus20phonology" class="col-sm-8">
    
      <span class="title">Estimating historical probabilities of natural and unnatural processes</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>To appear in Phonology</em>
      
      
        2020
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://ling.auf.net/lingbuzz/004299" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begusPhonology.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper presents a technique for estimating the influences of channel bias on phonological typology. The technique based on statistical bootstrapping enables the estimation of Historical Probability, the probability that a synchronic alternation arises based on two diachronic factors – the number of sound changes required for an alternation to arise and their respective probabilities. We estimate Historical Probabilities of six attested and unattested alternations targeting feature [voice], compare Historical Probabilities of these alternations, perform inferential statistics on the comparison, and compare outputs of the diachronic model against the independently observed synchronic typology to evaluate the performance of the channel bia approach. The proposed technique also identies mismatches in typological predictions of the analytic bias and channel bias approaches. By comparing these mismatches with the observed typology, this paper attempts to quantitatively evaluate the distinct contributions of the two influences on typology in a set of alternations targeting feature [voice].</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus19" class="col-sm-8">
    
      <span class="title">Post-nasal devoicing and the blurring process</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Journal of Linguistics</em>
      
      
        2019
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://www.cambridge.org/core/journals/journal-of-linguistics/article/postnasal-devoicing-and-the-blurring-process/51B1F27754D14F8BF523B905FFFE3BA1" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>This paper addresses one of the most contested issues in phonology: unnatural alternations. First, non-natural phonological processes are subdivided into unmotivated and unnatural. The central topic of the paper is an unnatural process: post-nasal devoicing (PND). I collect thirteen cases of PND and argue that in all reported cases, PND does not derive from a single unnatural sound change (as claimed in some individual accounts of the data), but rather from a combination of three sound changes, each of which is phonetically motivated. I present new evidence showing that the three stages are directly historically attested in the pre-history of Yaghnobi. Based on several discussed cases, I propose a new diachronic model for explaining unnatural phenomena called the Blurring Process and point to its advantages over competing approaches (hypercorrection, perceptual enhancement, and phonetic motivation). The Blurring Process establishes general diachronic conditions for unnatural synchronic processes and can be employed to explain unnatural processes beyond PND. Additionally, I provide a proof establishing the minimal sound changes required for an unmotivated/unnatural alternation to arise. The Blurring Process and Minimal Sound Change Requirement have implications for models of typology within the Channel Bias approach. This paper thus presents a first step toward the ultimate goal of quantifying the influences of Channel Bias on phonological typology.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begusnazarov18" class="col-sm-8">
    
      <span class="title">Gradient trends against phonetic naturalness: The case of Tarma Quechua</span>
      <span class="author">
        
          
            
              
                
                  Beguš, Gašper,
                
              
            
          
        
          
            
              
                
                  and Nazarov, Aleksei
                
              
            
          
        
      </span>

      <span class="periodical">
      
        <em>In NELS 48: Proceedings of the Forty-Eighth Annual Meeting of the North East Linguistic Society</em>
      
      
        2018
      
      </span>
    

    <span class="links">
    
    
    
    
      [<a href="/assets/pdf/begusNazarovNels.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2017</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus" class="col-sm-8">
    
      <span class="title">A Formal Model of Phonological Typology</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the West Coast Conference on Formal Linguistics</em>
      
      
        2017
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="http://search.proquest.com/docview/2244647771/" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus_17_a-formal-model.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>Identifying and modeling factors that influence typology has been one of the most contested issues in phonology with two major lines of thought emerging in this discussion: the Analytic Bias (AB) and Channel Bias (CB) approaches (Moreton 2008). Empirical evidence in favor of both approaches exists, yet very few attempts have been made to model them together. This paper aims to fill this gap and proposes a new MaxEnt-compatible model of phonological typology that models both AB and CB together. The first step towards a new model of typology is to establish quantitative models of each of the subcomponents: AB and CB. To encode the AB portion of the typology, we adopt Wilson’s (2006) approach of differentiating variance in the prior of a MaxEnt model of phonological learning; to encode the CB portion, we adopt Beguš’s (2016) new model of typology within CB that operates with Historical Probabilities of Alternations and an estimation method called Bootstrapping Sound Changes. This paper proposes a new model of typology that combines differentiating prior variance (AB; Wilson 2006) with estimating Historical Weights based on Historical Probabilities (CB; Beguš 2016), whereby both variables influence the typology. I further argue that this new model performs better than the current "split" models on the basis of two alternations, post-nasal voicing and devoicing, and point to future directions this line of research should take.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="doi:10.1121/1.5007728" class="col-sm-8">
    
      <span class="title">Effects of ejective stops on preceding vowel duration</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Journal of the Acoustical Society of America</em>
      
      
        2017
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="https://doi.org/10.1121/1.5007728" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus_effects_of_ejective_stops_on_preceding_vowel_duration_01.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>One of the most widely studied observations in linguistic phonetics is that, all else being equal, vowels are longer before voiced than before voiceless obstruents. The causes of this phonetic generalization are, however, poorly understood and several competing explanations have been proposed. No studies have so far measured vowel duration before stops with yet another laryngeal feature: ejectives. This study fills this gap and presents results from an experiment that measures vowel duration before stops with all three laryngeal features in Georgian and models effects of both closure and voice onset time (VOT) on preceding vowel duration at the same time. The results show that vowels have significantly different durations before all three series of stops, voiced, ejective, and voiceless aspirated, even when closure and VOT durations are controlled for. The results also suggest that closure and VOT durations are inversely correlated with preceding vowel duration. These results combined bear several implications for the discussion of causes of vowel duration differences: the data support the hypotheses that claim that laryngeal gestures, temporal compensation, and closure velocity affect vowel duration. Some explanations, especially perceptual and airflow expenditure explanations, are considerably weakened by the results.</p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2016</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus16" class="col-sm-8">
    
      <span class="title">The phonetics of the independent svarita in Vedic</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>In Proceedings of the 26th Annual UCLA Indo-European Conference</em>
      
      
        2016
      
      </span>
    

    <span class="links">
    
    
    
      [<a href="http://www.hempen-verlag.de/sprachwissenschaften/indogermanistik/ucla-proceedings/proceedings-of-the-26th-annual-ucla-indo-european-conference.html" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus_the_phonetics_of_independent_svarita_in_vedic.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
  </div>
</div>
</li></ol>

  <h2 class="year">2015</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus15" class="col-sm-8">
    
      <span class="title">A new rule in vedic metrics</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Journal of the American Oriental Society</em>
      
      
        2015
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="http://dx.doi.org/10.7817/jameroriesoci.135.3.541" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus_a_new_rule_in_vedic_metrics.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>In this paper I propose a new rule of Vedic meter. The glides v and y are regularly lost before the corresponding high vowels ū̆ and ī̆ in Vedic. I argue that the word-initial glides v and y before the short vowels ŭ and ĭ still “make position” and that they should be restored for metrical purposes. This means that word-final syllables of the shape -V̆C should be scanned long if the following syllable begins with a u- or i- that goes back to *vu- or *yi-. This new rule has consequences for the general metrical shape of the Rigveda, as cadences previously scanned as irregular will be repaired to their canonical shape. The rule can also be employed as etymologically decisive for words that can potentially go back to forms with or without an initial glide.</p>
    </span>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="Beguš2015" class="col-sm-8">
    
      <span class="title">The circumflex advancement in prekmurje Slovenian and Bednja Kajkavian</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Zeitschrift für Slawistik</em>
      
      
        2015
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="http://dx.doi.org/10.1515/slaw-2015-0003" target="_blank">HTML</a>]
    
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>The circumflex advancement is usually dated after the loss of the weak jers. However, this chronology has been questioned by Vermeer (1979) and Greenberg (1992, 1993), who claim the opposite: that the weak jers were lost after the advancement. They further propose the “non-advancement rule”, by which the circumflex does not advance if a weak jer follows. Their evidence comes almost exclusively from the l-participles of the accentual paradigm c, which have the initial accent in the two dialects. The article presents new data that argue against this proposal. It is shown that the circumflex regularly advances in words outside the category of l-participles despite the presence of a subsequent weak jer. Moreover, a new explanation is given for the initial accent in l-participles that better captures the data. </p>
    </span>
    
  </div>
</div>
</li></ol>

  <h2 class="year">2011</h2>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-2 abbr">
  
  </div>

  <div id="begus11" class="col-sm-8">
    
      <span class="title">Relativna kronologija naglasnih pojavov govora Žirovske kotline poljanskega narečja</span>
      <span class="author">
        
          
            
              Beguš, Gašper
            
          
        
      </span>

      <span class="periodical">
      
        <em>Slovenski jezik – Slovene Linguistic Studies</em>
      
      
        2011
      
      </span>
    

    <span class="links">
    
      [<a class="abstract">Abs</a>]
    
    
    
      [<a href="http://dx.doi.org/10.17161/SLS.1808.7536" target="_blank">HTML</a>]
    
    
      [<a href="/assets/pdf/begus_relativna_kronologija_naglasnih_pojavov_govora_zirovske_kotline_poljanskega_narecja.pdf" target="_blank">PDF</a>]
    
    
    
    
    
    </span>

    <!-- Hidden abstract block -->
    
    <span class="abstract hidden">
      <p>(translation) The Žiri Basin local dialect within the Poljane dialect evinces several special features in its structure and development. Based on the system described in Stanonik (1977), the author elucidates the system’s accentual changes from Common Slovene to the present state, and also presents some new discoveries concerning the system itself that help explain the phenomena more precisely. This description helps establish a relative chronology of the accentual phenomena, the resulting model of which is compared and contrasted with other explanations. Finally, the relative chronology of accentual changes is placed in the larger context of the development of Slovene.</p>
    </span>
    
  </div>
</div>
</li></ol>


</div>

  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2020 Gasper Begus.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    Last updated: December 08, 2020.
    
  </div>
</footer>



  </body>

  <!-- Load Core and Bootstrap JS -->
<script src="https://code.jquery.com/jquery-3.4.1.slim.min.js" integrity="sha384-J6qa4849blE2+poT4WnyKhv5vZF5SrPo0iEjwBvKU7imGFAV0wwj1yYfoRSJoZ+n" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.0/umd/popper.min.js" integrity="sha256-OH05DFHUWzr725HmuHo3pnuvUUn+TJuj8/Qz9xytFEw=" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.17.0/js/mdb.min.js"  integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>

<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />


<!-- Load KaTeX -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" integrity="sha256-V8SV2MO1FUb63Bwht5Wx9x6PVHNa02gv8BgH/uH3ung=" crossorigin="anonymous" />
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.js" integrity="sha256-F/Xda58SPdcUCr+xhSGz9MA2zQBPb0ASEYKohl8UCHc=" crossorigin="anonymous"></script>
<script src="/assets/js/katex.js"></script>



<!-- Load Mansory & imagesLoaded -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js" integrity="" crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>

<!-- Project Cards Layout -->
<script type="text/javascript">
  // Init Masonry
  var $grid = $('.grid').masonry({
    gutter: 10,
    horizontalOrder: true,
    itemSelector: '.grid-item',
  });
  // layout Masonry after each image loads
  $grid.imagesLoaded().progress( function() {
    $grid.masonry('layout');
  });
</script>





<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-173393077-1', 'auto');
ga('send', 'pageview');
</script>



</html>
